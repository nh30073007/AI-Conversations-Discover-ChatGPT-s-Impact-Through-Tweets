{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b584d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date                   id  \\\n",
      "0  2023-03-29 22:58:21+00:00  1641213230730051584   \n",
      "1  2023-03-29 22:58:18+00:00  1641213218520481805   \n",
      "2  2023-03-29 22:57:53+00:00  1641213115684536323   \n",
      "3  2023-03-29 22:57:52+00:00  1641213110915571715   \n",
      "4  2023-03-29 22:57:26+00:00  1641213003260633088   \n",
      "\n",
      "                                             content        username  \\\n",
      "0  [free, ai, marketing, automation, tool, strate...  RealProfitPros   \n",
      "1               [mecolehardman4, chat, gpt, say, 15]    AmyLouWho321   \n",
      "2  [chat, pdf, check, new, ai, quickly, answer, q...      yjleon1976   \n",
      "3  [ai, mus, court, life, must, face, judge, dest...  ChatGPT_Thinks   \n",
      "4  [people, havent, heard, chat, gpt, yet, first,...   nikocosmonaut   \n",
      "\n",
      "   like_count  retweet_count  \n",
      "0         0.0            0.0  \n",
      "1         0.0            0.0  \n",
      "2         0.0            0.0  \n",
      "3         0.0            0.0  \n",
      "4         0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "#PREPROCESS STEP \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "# DATASET\n",
    "df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\500k ChatGPT-related Tweets Jan-Mar 2023\\Twitter Jan Mar.csv')\n",
    "\n",
    "# CONVERT TO LOWERCASE\n",
    "df['content'] = df['content'].str.lower()\n",
    "\n",
    "# REMOVE URLS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "\n",
    "# TOKENIZATION\n",
    "df['content'] = df['content'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# STOPWORD REMOVAL\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['content'] = df['content'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# LEMMATIZATION\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['content'] = df['content'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3165e2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d2aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date                   id  \\\n",
      "0  2023-03-29 22:58:21+00:00  1641213230730051584   \n",
      "1  2023-03-29 22:58:18+00:00  1641213218520481805   \n",
      "2  2023-03-29 22:57:53+00:00  1641213115684536323   \n",
      "3  2023-03-29 22:57:52+00:00  1641213110915571715   \n",
      "4  2023-03-29 22:57:26+00:00  1641213003260633088   \n",
      "\n",
      "                                             content        username  \\\n",
      "0  [free, ai, marketing, automation, tool, strate...  RealProfitPros   \n",
      "1               [mecolehardman4, chat, gpt, say, 15]    AmyLouWho321   \n",
      "2  [chat, pdf, check, new, ai, quickly, answer, q...      yjleon1976   \n",
      "3  [ai, mus, court, life, must, face, judge, dest...  ChatGPT_Thinks   \n",
      "4  [people, havent, heard, chat, gpt, yet, first,...   nikocosmonaut   \n",
      "\n",
      "   like_count  retweet_count emojis  \n",
      "0         0.0            0.0     []  \n",
      "1         0.0            0.0     []  \n",
      "2         0.0            0.0     []  \n",
      "3         0.0            0.0     []  \n",
      "4         0.0            0.0     []  \n"
     ]
    }
   ],
   "source": [
    "# finding emoji \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "\n",
    "# DATASET\n",
    "df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\500k ChatGPT-related Tweets Jan-Mar 2023\\Twitter Jan Mar.csv')\n",
    "\n",
    "# CONVERT TO LOWERCASE\n",
    "df['content'] = df['content'].str.lower()\n",
    "\n",
    "# REMOVE URLS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "\n",
    "# TOKENIZATION\n",
    "df['content'] = df['content'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# STOPWORD REMOVAL\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['content'] = df['content'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# LEMMATIZATION\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['content'] = df['content'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# EMOJI EXTRACTION\n",
    "def extract_emojis(text):\n",
    "    return [c for c in text if emoji.is_emoji(c)]\n",
    "\n",
    "df['emojis'] = df['content'].apply(lambda x: extract_emojis(x))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a79f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d673a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total retweets: 103452\n",
      "Positive retweets: 3136\n",
      "Negative retweets: 754\n",
      "Positive percentage: 3.031357537795306\n",
      "Negative percentage: 0.7288404284112439\n"
     ]
    }
   ],
   "source": [
    "#count the number of retweets and calculate the percentage of positive and negative retweets\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "\n",
    "# DATASET\n",
    "df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\500k ChatGPT-related Tweets Jan-Mar 2023\\Twitter Jan Mar.csv')\n",
    "\n",
    "# CONVERT TO LOWERCASE\n",
    "df['content'] = df['content'].str.lower()\n",
    "\n",
    "# REMOVE URLS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "\n",
    "# TOKENIZATION\n",
    "df['content'] = df['content'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# STOPWORD REMOVAL\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['content'] = df['content'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# LEMMATIZATION\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['content'] = df['content'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# EMOJI EXTRACTION\n",
    "def extract_emojis(text):\n",
    "    return [c for c in text if emoji.is_emoji(c)]\n",
    "\n",
    "df['emojis'] = df['content'].apply(lambda x: extract_emojis(x))\n",
    "\n",
    "# IDENTIFY POSITIVE AND NEGATIVE KEYWORDS\n",
    "positive_keywords = ['good', 'happy', 'awesome']\n",
    "negative_keywords = ['bad', 'sad', 'terrible']\n",
    "\n",
    "# COUNT RETWEETS\n",
    "retweet_count = len(df[df['retweet_count'] > 0])\n",
    "\n",
    "# COUNT POSSITIVE RETWEETS\n",
    "positive_retweets = df[df['retweet_count'] > 0]['content'].apply(lambda x: any(keyword in x for keyword in positive_keywords)).sum()\n",
    "\n",
    "# COUNT NEGATIVE RETWEETS\n",
    "negative_retweets = df[df['retweet_count'] > 0]['content'].apply(lambda x: any(keyword in x for keyword in negative_keywords)).sum()\n",
    "\n",
    "# CALCULATE PSOITIVE NEGATIVE RETWEETS PERSENTAGE\n",
    "positive_percentage = (positive_retweets / retweet_count) * 100\n",
    "negative_percentage = (negative_retweets / retweet_count) * 100\n",
    "\n",
    "print(\"Total retweets:\", retweet_count)\n",
    "print(\"Positive retweets:\", positive_retweets)\n",
    "print(\"Negative retweets:\", negative_retweets)\n",
    "print(\"Positive percentage:\", positive_percentage)\n",
    "print(\"Negative percentage:\", negative_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6615cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b23aa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets: 264036\n",
      "Number of negative tweets: 79912\n",
      "Number of neutral tweets: 156088\n",
      "                        date                   id  \\\n",
      "0  2023-03-29 22:58:21+00:00  1641213230730051584   \n",
      "1  2023-03-29 22:58:18+00:00  1641213218520481805   \n",
      "2  2023-03-29 22:57:53+00:00  1641213115684536323   \n",
      "3  2023-03-29 22:57:52+00:00  1641213110915571715   \n",
      "4  2023-03-29 22:57:26+00:00  1641213003260633088   \n",
      "\n",
      "                                             content        username  \\\n",
      "0  [free, ai, marketing, automation, tool, strate...  RealProfitPros   \n",
      "1               [mecolehardman4, chat, gpt, say, 15]    AmyLouWho321   \n",
      "2  [chat, pdf, check, new, ai, quickly, answer, q...      yjleon1976   \n",
      "3  [ai, mus, court, life, must, face, judge, dest...  ChatGPT_Thinks   \n",
      "4  [people, havent, heard, chat, gpt, yet, first,...   nikocosmonaut   \n",
      "\n",
      "   like_count  retweet_count  sentiment_polarity  \n",
      "0         0.0            0.0              0.5106  \n",
      "1         0.0            0.0              0.0000  \n",
      "2         0.0            0.0              0.7184  \n",
      "3         0.0            0.0              0.0000  \n",
      "4         0.0            0.0              0.0258  \n"
     ]
    }
   ],
   "source": [
    "# EXTRACT SENTIMENT ANALYSIS USING VADER\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# DATASET\n",
    "df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\500k ChatGPT-related Tweets Jan-Mar 2023\\Twitter Jan Mar.csv')\n",
    "\n",
    "# CONVERT TO LOWERCASE\n",
    "df['content'] = df['content'].str.lower()\n",
    "\n",
    "# REMOVE URLS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "# REMOVE SPECIAL CHARACTERS\n",
    "df['content'] = df['content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "\n",
    "# TOKENIZATION\n",
    "df['content'] = df['content'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# STOPWORD REMOVAL\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['content'] = df['content'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# LEMMATIZATION\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['content'] = df['content'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# SENTIMENT ANALYSE USING VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment_polarity'] = df['content'].apply(lambda x: sia.polarity_scores(' '.join(x))['compound'])\n",
    "\n",
    "\n",
    "positive_tweets = df[df['sentiment_polarity'] > 0]\n",
    "negative_tweets = df[df['sentiment_polarity'] < 0]\n",
    "neutral_tweets = df[df['sentiment_polarity'] == 0]\n",
    "\n",
    "print(\"Number of positive tweets:\", len(positive_tweets))\n",
    "print(\"Number of negative tweets:\", len(negative_tweets))\n",
    "print(\"Number of neutral tweets:\", len(neutral_tweets))\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b6905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6296293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date                   id  \\\n",
      "0  2023-03-29 22:58:21+00:00  1641213230730051584   \n",
      "1  2023-03-29 22:58:18+00:00  1641213218520481805   \n",
      "2  2023-03-29 22:57:53+00:00  1641213115684536323   \n",
      "3  2023-03-29 22:57:52+00:00  1641213110915571715   \n",
      "4  2023-03-29 22:57:26+00:00  1641213003260633088   \n",
      "\n",
      "                                             content        username  \\\n",
      "0  Free AI marketing and automation tools, strate...  RealProfitPros   \n",
      "1           @MecoleHardman4 Chat GPT says it’s 15. 😂    AmyLouWho321   \n",
      "2  https://t.co/FjJSprt0te - Chat with any PDF!\\n...      yjleon1976   \n",
      "3  AI muses: \"In the court of life, we must all f...  ChatGPT_Thinks   \n",
      "4  Most people haven't heard of Chat GPT yet.\\nFi...   nikocosmonaut   \n",
      "\n",
      "   like_count  retweet_count                                  hashtags  \\\n",
      "0         0.0            0.0                                 [ChatGPT]   \n",
      "1         0.0            0.0                                        []   \n",
      "2         0.0            0.0              [research, chatpdf, ChatGPT]   \n",
      "3         0.0            0.0  [OutOfContextAI, AILifeLessons, ChatGPT]   \n",
      "4         0.0            0.0                                        []   \n",
      "\n",
      "   hashtag_count  \n",
      "0              1  \n",
      "1              0  \n",
      "2              3  \n",
      "3              3  \n",
      "4              0  \n"
     ]
    }
   ],
   "source": [
    "#EXTRACT HASHTAGs\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# DATASET\n",
    "df = pd.read_csv(r'C:\\Users\\nh013\\Desktop\\500k ChatGPT-related Tweets Jan-Mar 2023\\Twitter Jan Mar.csv')\n",
    "\n",
    "# EXTRACT HASTAGS \n",
    "df['hashtags'] = df['content'].apply(lambda x: re.findall(r'#(\\w+)', str(x)))\n",
    "\n",
    "# COUNT NUMBER OF HASTAGS OVER EACH TWEETS\n",
    "df['hashtag_count'] = df['hashtags'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1528aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1122b4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
